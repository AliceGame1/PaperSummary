# 小白都能懂得信息论：渐近均分性

第三章 渐近均分性Asymptotic equipartition property (AEP)
 最近在学习第三章，觉得非常有意思，因此想做个简单的学习总结，就当是笔记啦。废话不说，进
 入主题。
 我们先说结论，第三章主要是围绕渐近均分性和典型集来展开讨论的。就说了这么一件事：
 当码长序列足够长时，其中一部分序列就会显示出某种固定性质，(可类比抛硬币当次数越大正反
 各接近一半): 各个符号出现的频率接近于概率，而这些序列的的概率则趋近于相等，且它们的和非常接近于1。这些序列就是“典型序列”。其余不具备这种性质的序列，我们称之为非典型序

 列，这些非典型序列的出现概率之和接近于零。序列的长度越长，典型序列的总概率就越接近于1,它的各个序列的出现概率越趋近于相等。而我们就把这种现象称之为渐近均分性，等概嘛，那跟均分的道理就是一样嘛，顾名思义，所以很好理解吧。

 那接下来我们详细说明一下这其中的关系，如何从均分性到典型集递进的？包括数据的压缩映射，
 到底是怎么压缩的。
 那么什么是渐近均分性呢？渐近均分性到底讲了个啥呢？我们为啥要有渐近均分性这个概念呢？
 (其实结论已经暴露它的用途了)
 先单纯从数学式子角度回答这个问题：
 斩近均分性其实主要就是说了一件事情：$\frac1{\mathrm{n}}\log\frac1{p(X_1,X_2,...,X_n)}$ 近似等于熵H，其中
 $X_1,X_2,\ldots X_n$ 为i.i.d随机变量，$p(X_1,X_2,\ldots,X_n)$ 是随机变量序$X_1,X_2,\ldots X_n$出现的
 概率。当n很大的时候 (这里其实利用了弱大数定律), 一个观察序列$p(X_1,X_2,\ldots,X_n)$ 近似等于$2^{-\mathrm{n}H}$。

上述中出现的近似这个词，我们就需要解释一下了，这个近似其实本质上就是依概率收敛到H。我知道，一定有很多人对收敛这个概念晕。那么为了更好解释渐近均分性定理，我们先把涉及到收敛，大数定律这些概念解释清楚。首先是收敛，几乎处处收敛，依概率收敛？(其实在数学分析上，我们针对级数和函数列还会有条件收敛，绝对收敛，一致收敛，近一致收敛，内闭一致收敛， 测度收敛，平均收敛，弱收敛。这里因为用不到，暂时不介绍了，后面用到了再添加)。我们假设有 $\mathrm{a}_1,\mathrm{a}_2,\ldots a_n,\ldots$ 是一个随机变量序列，a是一个常数。

 1.收敛：
 $\forall\varepsilon>0\exists N\in N^+$,当$\mathrm{n> N}$时，$|\mathrm{a_n- a|< \varepsilon}$,即$\lim_{n\to\infty}|a_n-a|=0$,记作$a_n\to a$
 ,我们称数列$a_n$收敛于a.(PS.为了方便起见，在这里我是把$\mathrm{a}_1,\mathrm{a}_2,\ldots a_n,\ldots$看成普通数列引入的收敛定义。)

 这和数列极限就是等价的。用图可能会更直观吧！

![img](C:/Users/leanyxliu/Desktop/%E4%BF%A1%E6%81%AF%E8%AE%BA%20%E5%85%B8%E5%9E%8B%E9%9B%86/asset/v2-3b58b570133404e12636c54c0a09a3c0_1440w.webp)

2.几乎确定收敛： $\forall\varepsilon>0$,$\exists N\in N^+$,当$\mathrm{n> N}$时，$P( |\mathrm{a} _n- $a$|< \varepsilon) = 1$, 即

=1, 我们称$a_n$几乎确定收敛于$a.$ 
$$
P\left(\lim_{\mathrm{n}\to\infty}|a_n-a|\right)=1
$$
 直观的图表示就是：

![img](C:/Users/leanyxliu/Desktop/%E4%BF%A1%E6%81%AF%E8%AE%BA%20%E5%85%B8%E5%9E%8B%E9%9B%86/asset/v2-962354457e794307defc8adcab9cf63e_1440w.webp)

3.依概率收敛： $\delta>0$,$\exists N\in N^+$,当$\mathrm{n> N}$时，$|P( |\mathrm{a} _n- $a$|< \varepsilon) - 1|\leq \delta$,即
 $\begin{aligned}\lim_{n\to\infty}P\left(|a_n-a|\leq\varepsilon\right)=1\mathrm{or}\lim_{n\to\infty}P\left(|a_n-a|>\varepsilon\right)=0\end{aligned}$
 记作 $a_n\to a$ ,我们称 $a_n$ 依概率收敛与a.
 同样地直观图表示为：

![img](C:/Users/leanyxliu/Desktop/%E4%BF%A1%E6%81%AF%E8%AE%BA%20%E5%85%B8%E5%9E%8B%E9%9B%86/asset/v2-c0e4affe8d057ea16ae0876dbb4872c8_1440w.webp)

其实啊，依概率收敛是测度论中的依测度收敛概念在概率论中的特例。(后期补上依测度收敛) 。
那接下来我们来捋一捋强大数定律和弱大数定律的关系，为了方便理解，这里先放一个大数定律的概念。
 大数定理：就是说如果重复做相同试验，做的试验次数越多，某事件发生的频率就越趋向于该事件
 发生的概率，或者从稳定性角度来说：当对一个随机变量进行无限次采样时，得到的平均值会无限
 强弱大数定律要单纯从定义的式子上看，那就是极限位置放在哪里的事，但这个极限位置放在哪
 里，意义可差多去了。
 假设$X_1,X_2,\ldots X_n$是i.i.d的随机变量序列，且$E\left(X_{\mathrm{k}}\right)=\mu,\frac{\sum_{i=1}^{n}X_{i}}{n}=\mu_{n}$ 。
 强大数定律：

$$
\mathrm{P}\left(\lim_{n\to\infty}|\mu_n-\mu|<\varepsilon\right)=1
$$
 上述这个式子降维到幼儿园水平解释的话：
 从括号里面看当$n\to\infty$ 时，$\mu_n$ 几乎一定能不断的向 $\mu$ 直直飞奔而去，就跟你们在机场见到男
 票那样飞奔的路线，越靠近对方，方向越不会出错。用我们美丽的数学语言表示其实就是：
 $\forall\varepsilon> 0, |\mu_n- \mu|\leq \varepsilon, $n$\to \infty\Rightarrow \mu_n\approx \mu$ 这件事它一定会发生。
 弱大数定律：
 $\forall\varepsilon>0,\operatorname*{lim}_{n\to\infty}\mathcal{P}\left(|\mu_{n}-\mu|<\varepsilon\right)=1$

$\forall\varepsilon>0,\operatorname*{lim}_{n\to\infty}\mathcal{P}\left(|\mu_{n}-\mu|<\varepsilon\right)=1$ 着上式子，极限在概率外面。说明是对概率求得极限，表达是某个事件发生趋势的问题。翻译成人
 话就是：当$n\to\infty$ 时，$\mu_n$ 越接近 $\mu$ 的可能性越来越大，这就跟你跑马拉松，一直稳居第一，
 你最终很可能是冠军的情形一样，我们只能说随着时间的推移，你一直是第一，那你是冠军可能性越大，但不代表你一定是，你还是有极小概率被黑马超越的。再用我们美到爆炸的数学语言表达出来就是：$\forall\varepsilon> 0, |\mu_n- \mu|\leq \varepsilon, $n$\to \infty\Rightarrow \mu_n\approx \mu$ 这件事发生的可能性越来越大，

 这里补充一些其它课本上关于强弱大数定律的等价定义(其实都一样，没啥区别，就是形式整的花
 里胡哨而已):
 强大数定律：P $\left(\lim_{n\to\infty}|\mu_n-\mu|<\varepsilon\right)=1$
 弱大数定律$:\quad\forall\varepsilon>0,\forall\delta>0\lim_{n\to\infty}|\mathcal{P}\left(|\mu_n-\mu|<\varepsilon\right)-1|\leq\delta$
 总结：是不是觉得强大数定律很牛叉！(废话，因为人家已经说了自己强嘛。)
 至此，我们收敛，大数定律的讲解全部over了，我也不知道我有没有讲清楚，如果有啥错误还希
 望大家指正。接下来我们回归到主题AEP。
 因此，为了描述更精确，信息论的大佬们把AEP本质写成定理就是：
 定理3.1.1 (AEP) 若$X_1,X_2,\ldots X_n$为i.i.d $\sim p(x)$ ,则

$\frac{1}{\mathrm{n}}\log\frac{1}{p(X_{1},X_{2},...,X_{\mathrm{n}})}\to H(X)$ 依概率。这个定理其实证明非常简单，Thomas M.cover虽然上面有详细的证明。但为了方便大家，我还是
 贴出来吧。毕竟我是一个很贴心的人。哈哈。
 证明：独立随机变量的函数还是独立随机变量。因此，由于$X_i$ 是 i.i. d 的，因此，$\log p\left(X_i\right)$
 也是 i.i.d,因而，由弱大数定律，我们有

$$
-\frac{1}{\text{n}}\mathrm{logp}\left(X_1,X_2,\ldots X_n\right)=-\frac{1}{n}\sum_{i}\log p\left(X_i\right)-E\log p\left(X\right)=H\left(X\right)
$$
 其中第二个等式是利用弱大数定律：依概率收敛作用的结果。
 证毕。
 接下来我们回答第二个问题，为啥我们有渐近均分性这个东西，它到底有啥用，前方高能，建议大
 脑网速流畅下观看哦！
 渐近均分性是用来区分典型集和非典型集！!!!!
 我们通过上述巴拉巴拉一大堆引出了渐近均分性，它本质上是基于大数定律。
 那么，现在我们的把问题转到我们信息论中的码长序列，也就是开头给出的结论，当码长序列足够
 长时，其中一部分序列就会显示出某种固定性质，(可类比抛硬币当次数越大正反各接近一半) : 各个符号出现的频率接近于概率，而这些序列的的概率则趋近于相等，且它们的和非常接近于1。这些序列就是“典型序列”。其余不具备这种性质的序列，我们称之为非典型序列，这些非典型序

 列的出现概率之和接近于零。序列的长度越长，典型序列的总概率就越接近于1，它的各个序列的出现概率越趋近于相等。而我们就把这种现象称之为渐近均分性。

 那么啥叫典型序列呢？那我们接下来给出定义
 典型集定义为：
 设 $X_1,X_2,\ldots X_n$ 是概率密度函数为 $p(x)$ 的i.i.d随机序列，满足以下条件：

$$
\frac{1}{\mathrm{n}}\mathrm{log}p\left(X_{1},X_{2},\ldots,X_{\mathrm{n}}\right)-H(X)|\leq\varepsilon
$$

$$
\Rightarrow2^{-\mathrm{n}\left(H(X)+\varepsilon\right)}\leq p\left(X_1,X_2,\ldots,X_\mathrm{n}\right)\leq2^{-\mathrm{n}\left(H(X)-\varepsilon\right)}
$$
 则称源字母序列为典型序列 (典型集), 记为 $A_\varepsilon^{(n)}$ 。
 有没有很熟悉上面的公式？? 渐近均分性定理的变性啊！! 唉，不得不感叹，数学真是一个美妙的
 工具！!现在知道渐近均分性的是如何应用到典型集里面了吧。(我可真优秀) 。
 到这里可能有很多人会问，你说了这么多，我们要典型集干嘛呢。我们在信息论后面的学习过程中
 要用它来证明信源编码定理和信道编码定理，分别对应的是弱典型集和强典型集。那它为啥可以做到呢？这是因为典型集它具有非常特殊的一些性质，具体是：

 定理3.1.2
 1.如果 $x_1,x_2,\ldots,x_n\in A_\varepsilon^{(n)}$ ,则
 $H(X)-\varepsilon\leq-\frac1n\log p(x_1,x_2,\ldots,x_n)\leq H(X)+\varepsilon$ 。
 2.当n充分大时，$\Pr\{A_\varepsilon^{(n)}\}>1-\varepsilon$ 。
 3. $|A_\varepsilon^{(n)}|\leq2^{\mathrm{n}(H(X)+\varepsilon)}$ , 代表典型集中元素个数。

4.当n充分大时，$|A_\varepsilon^{(n)}|\geq(1-\varepsilon)\:2^{\mathrm{n}(H(X)-\varepsilon)}$。为了更好的帮助大家理解典型集高概的一些性质，这里手动给大家展示一下具体的证明过程 (我内
 心是想偷懒的，哈哈)
 1.证明：
 这个性质就是典型集定义变形的得到的，上面白纸黑字写的很清楚了，那么这里我还是要写一下：
 根据典型集的定义，我们有：

$$
2^{-\mathrm{n}(H(X)+\varepsilon)}\leq p\left(X_1,X_2,\ldots,X_\mathrm{n}\right)\leq2^{-\mathrm{n}(H(X)-\varepsilon)}
$$

$$
-\frac1{\mathrm{n}}\log p\left(X_1,X_2,\ldots,X_\mathrm{n}\right)-H(X)|\leq\varepsilon
$$

$$
\Rightarrow H(X)-\varepsilon\leq-\frac{1}{\mathrm{n}}\mathrm{log}p\left(X_{1},X_{2},\ldots,X_{\mathrm{n}}\right)\leq H(X)+\varepsilon 
$$
 证毕
 2. 证明：

这条性质，我觉得大家如果看懂了AEP定理的证明或者说理解了啥叫依概率收敛的概念，估计计眼睛都可以写出来吧。好吧，我们给穿官方版的：
 由定理3.1.1 (AEP),我们知道
 $-\frac{1}{\mathrm{n}}\operatorname{logp}\left(X_{1},X_{2},\ldots X_{n}\right)=-\frac{1}{n}\sum_{i}\operatorname{log}p\left(X_{i}\right)-E\operatorname{log}\operatorname{p}\left(X\right)=H\left(X\right)$
 在上述我们说了其中第二个等式是利用弱大数定律：依概率收敛作用的结果。
 我们把上述公式降维一下，依概率这里补充完整，虽然前面已经解释的很清楚了，根据依概率
 的定义：
 $\lim_{\mathrm{n}\to\infty}P\left(|-\frac{1}{\mathrm{n}}\mathrm{logp}\left(X_1,X_2,\ldots X_n\right)-H(X)|\right)=1$
 or$\lim _{\mathrm{n} \to \infty}P\left ( |- \frac 1{\mathrm{n} }\mathrm{logp}\left ( X_1, X_2, \ldots X_n\right ) - H( X) |> \varepsilon\right ) = 0$
 我们用数学分析经典的 $\varepsilon-\delta$ 语言描述出来就是：

$$
>0\exists N\in N^+\text{n}>N,P\left|\left(\left|-\frac1{\mathrm{n}}\mathrm{logp}\left(X_1,X_2,\ldots X_n\right)-\mathrm{H(X)}\right|<\varepsilon\right)-\right. 
$$

$<\delta$, $1-\delta<P\left(|-\frac{1}{\mathrm{n}}\mathrm{logp}\left(X_1,X_2,\ldots X_n\right)-\mathrm{H}(\mathrm{X})|<\varepsilon\right)\leq1$
 取 $\delta_\mathrm{=E}$
 $1-\varepsilon<P\left(\left|-\frac{1}{\mathrm{n}}\mathrm{logp}\left(X_1,X_2,\ldots X_n\right)-\mathrm{H}(\mathrm{X})\right|<\varepsilon\right)\leq1$
 $\Rightarrow P\left(|-\frac{1}{\mathrm{n}}\mathrm{logp}\left(X_1,X_2,\ldots X_n\right)-\mathrm{H}(\mathrm{X})|<\varepsilon\right)>1-\varepsilon$ $\Rightarrow P\left(A_{\varepsilon}^{(\mathrm{n})}\right)>1-\varepsilon$

这里 $\varepsilon$ 是任意小的哦，因此概率非常接近1了，还不高概吗？香不香？ 证毕
 3.证明：
 由概率论最基本的公理化定义中的规范性，我们知道，全体事件发生的概率之和为1.因此，我们有

$$
\sum_{X\in\mathcal{X}^n}p(x)=1\geq\sum_{X\in A_\varepsilon^{(\mathrm{n})}}p(x)
$$
 又根据典型集的定义，我们知道典型集中的每个序列发生是等概的，且上下界也由定义给出了，即
 $2^{-\mathrm{n}(H(X)+\varepsilon)}\leq p\left(X_{1},X_{2},\ldots,X_{\mathrm{n}}\right)\leq2^{-\mathrm{n}(H(X)-\varepsilon)}$
 因此，我们有：

$$
\sum_{X\in\mathcal{X}^n}p(x)=1\geq\sum_{X\in A_\varepsilon^{(n)}}p(x)\geq\sum_{X\in A_\varepsilon^{(n)}}2^{-\mathrm{n}(H(X)+\varepsilon)}\to=2^{-\mathrm{n}(H(X)+\varepsilon)}\left|A_\varepsilon^{(n)}\right|
$$
 最后一个等式是因为等该求和嘛，有多少个，就加多少个了。因此我们有

$$
\left|A_\varepsilon^{(n)}\right|\leq2^{\mathrm{n}(H(X)+\varepsilon)}
$$
 完美的证毕了。
 Remark: 第三个这个证明，我一开始还是没反应过来的，因为我当时的思路，总是想着典型集去
 了，没有整体的格局观，忽略了最基本的概率知识，即：全体空事件发生的概率之和是1！!你看往往最简单的，我们最容易忽略的！那句话昨说来着，把最难的东西用最简单的方法解释清楚，才是最牛的 (怎么感觉像是在说我？)

 4. 证明：
 你看这第四个结论，当n充分大时$|A_\varepsilon^{(n)}|\geq(1-\varepsilon)2^{\mathrm{n}(H(X)-\varepsilon)}$不等式右边出现了$(1-\varepsilon)$ 我就
  猜到它肯定跟性质2：$P\{A_\varepsilon^{(n)}\}>1-\varepsilon$ 有关，为啥？长得一样啊。其实，绕来绕去，就是定义。

根据性质2，我们有：

$$
P\{A_\varepsilon^{(n)}\}>1-\varepsilon 
$$
 根据典型集定义，我们又有：

$$
2^{-\mathrm{n}(H(X)+\varepsilon)}\leq p\left(X_1,X_2,\ldots,X_\mathrm{n}\right)\leq2^{-\mathrm{n}(H(X)-\varepsilon)}
$$
 因此，

$$
P\{A_\varepsilon^{(n)}\}>1-\varepsilon\Rightarrow\sum_{X\in A_\varepsilon^{(n)}}2^{-\mathrm{n}(H(X)-\varepsilon)}\geq1-\varepsilon\Rightarrow\left|A_\varepsilon^{(n)}\right|
$$
 $\Rightarrow\left|A_{\varepsilon}^{(n)}\right|\geq\left(1-\varepsilon\right)2^{\mathrm{n}(H(X)-\varepsilon)}$
 不知道大家发现了没有，在整个证明包括渐近均分性，我们无数次用了基本的定义，依概率收敛也
 是。其实，在所有的知识体系里，我觉得定义永远是最重要的的，它比公理重要的多，因为定义告诉了我们这个东西是啥，一切的公理都是围绕它转开的，这句话是我本科数学分析老师给我说的。送给大家共勉。

 证毕
 关于整个性质这里我需要做个remark，就是，典型集的个数问题。这里会在接下来的数据压缩用
 到，我当时卡在个数问题里面很久，有幸得到了代彬老师的指点。如果全空间有n个元素，那么根据典型集性质它大概会有 $2^{\mathrm{n}H}$个，整个空间序列，是 $2^\mathrm{n}$ 个，然后你会发现一个问题

$$
\textup{lim}_{\infty}\frac{2^{nH(X)}}{2^{n}}=\operatorname*{lim}_{\mathrm{n}\to\infty}2^{n(H(X)-1)}=0\left(H\left(X\right)\leq1\right)
$$
 ${\mathrm{n}}^{--}\infty$ 这说明啥？说明，典型集的个数相对全空间来说，它非常非常少，但是，你不能说它没有哈。写到
 这里你们有没有觉得，典型集很神奇，它非常少，但却是高概率出现的！!我们正式用它的这个性质，来进行信源和信道编码定理的！! Amazing 不？我看到这个结论的时候还是挺震撼的，觉得C.E香农和B.麦克米伦脑子怎么就这么好使呢，哎，这肯能就是人家为啥是天才的原因吧，奥，他俩是一个发现均分性而另一一个是给出严格证明的。

另外一个还需要再啰嗦一句：典型序列，与大数律不同的是，并不是几乎处处落入典型集，即落入典型集元素个数的概率不为1，即不是全部，而是 2�(�(�)±�) 个。

这个我们刚刚也说了，个数非常小。我们说的高概率，指的是序列整个落入典型集的概率趋近于1.

接下来我们进入本章的第二个内容：**数据压缩**。他本质就是渐近均分性的一个推论，那我们来看看它到底讲了啥。

先说结论：数据压缩就是把一个完整的概率空间缩小到典型集，而典型集又非常小相对整个空间来说。它起到了压缩信息的作用，消除冗余，得以实现高效率传输。

数据压缩就是利用了**典型集的个数少，且高概率出现的性质，对信源进行编码。**

![img](C:/Users/leanyxliu/Desktop/%E4%BF%A1%E6%81%AF%E8%AE%BA%20%E5%85%B8%E5%9E%8B%E9%9B%86/asset/v2-678589c02ba7c9d1c6f3c29d1fa8cef2_1440w.webp)

![img](C:/Users/leanyxliu/Desktop/%E4%BF%A1%E6%81%AF%E8%AE%BA%20%E5%85%B8%E5%9E%8B%E9%9B%86/asset/v2-c3adea2468c6600c12d97b22fe09909d_1440w.webp)

设$X_1,X_2,\ldots X_n$ 是概率密度函数为 $p(x)$ 的i.i.d随机变量，为编码随机变量序列，将 $X^\mathrm{n}$ 的序列划分：典型集 $A_\varepsilon^{(n)}$ 及其补集，如上图3.1所示。
 数据压缩：
 1.将集合元素按某种顺序 (比如字典) 排列，
 2.指定下标可表示 $A_\varepsilon^{(n)}$ 中的每个序列。
 3.$|A_\varepsilon^{(n)}|\leq2^{n(H+\varepsilon)}$需要$n\left(H+\varepsilon\right)+1$ 比特 (额外的一比特是为了取整) 。
 4.编码加O，则编码 $A_\varepsilon^{(n)}$ 需要 $n\left(H+\varepsilon\right)+2$ 需要如图3-2所示)
 5.同理，对于非典型集 $A_\varepsilon^{(n)^c}$ 需要 $n\log|\mathcal{X}|+1$ 比特，编码加1需要 $n\log|\mathcal{X}|+2$比特。
 6.获得 $\chi^\mathrm{n}$ 一个编码方案。
 有没有注意到为何 $A_\varepsilon^{(n)^c}$ 所需要的比特数是利用全空间序列来计算的？?? 这里就是前文提到
 的，典型序列本质上占全空间非常非常少的一部分，因此我们可以非典型集可以近似利用全空间序
 列来计算。虽然会有误差，但这里误差可以忽略。
 利用上述的编码方式，我们就可以得到以下平均码长编码定理。
 定理3.2.1: 设 $X^\mathrm{n}$ 为服从 $p(x)$ 的i.i.d序列，$\varepsilon>0$ 则存在一个编码将长度为n的序列 $x^n$ 映射
 为比特串，使得映射是1-1的 (所以可逆),且对于充分大的n，我们有

 因此从平均意义上来说，可以用 $nH\left(X\right)$ 比特表示序列 $X^\mathrm{n}$
 本章最后一个内容高概率集与典型集
 我们知道由 $A_\varepsilon^{(n)}$ 的定义，它是高概率的小集合。